{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_on_CelebA_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNAcTbD+6SFnf/dJ3/wRVHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalburgul/MLDS_Learning/blob/master/Pytorch/GANs/DCGAN_on_CelebA_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPLX48TckxML"
      },
      "source": [
        "# **DCGAN (Deep Convollutional Generative Adversarial Networks)**\n",
        "\n",
        "Genertaing new celebrities after showing it pictures of many real celebrities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM9XyAKJl0Fm"
      },
      "source": [
        "## **Introduction to Generative Modeling**\n",
        "\n",
        "Deep neural networks are used mainly for supervised learning: classification or regression. Generative Adversarial Networks or GANs, however, use neural networks for a very different purpose: Generative modeling\n",
        "\n",
        "Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. [Source](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
        "\n",
        "Here we will train a generative adversarial network (GAN) to generate new celebrities after showing it pictures of many real celebrities. Most of the code here is from the dcgan implementation.\n",
        "\n",
        "While there are many approaches used for generative modeling, a Generative Adversarial Network takes the following approach:\n",
        "\n",
        "<img src=\"https://i.imgur.com/6NMdO9u.png\" style=\"width:420px; margin-bottom:32px\"/>\n",
        "\n",
        "\n",
        "There are two neural networks: a Generator and a Discriminator. The generator generates a \"fake\" sample given a random vector/matrix, and the discriminator attempts to detect whether a given sample is \"real\" (picked from the training data) or \"fake\" (generated by the generator). Training happens in tandem: we train the discriminator for a few epochs, then train the generator for a few epochs, and repeat. This way both the generator and the discriminator get better at doing their jobs.\n",
        "\n",
        "GANs however, can be notoriously difficult to train, and are extremely sensitive to hyperparameters, activation functions and regularization.\n",
        "\n",
        "\n",
        "**DCGAN** \n",
        "\n",
        "- A DCGAN is a direct extension of the GAN described above, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively. The discriminator is made up of strided convolution layers, batch norm layers, and LeakyReLU activations. \n",
        "\n",
        "- The input is a 3x64x64 input image and the output is a scalar probability that the input is from the real data distribution. \n",
        "- The generator is comprised of convolutional-transpose layers, batch norm layers, and ReLU activations with input of the latent vector, zz, that is drawn from a standard normal distribution and the output is a 3x64x64 RGB image.\n",
        "- The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdu2LGAOq7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b4bd0b-d3cc-49a7-9d08-500eeb51650f"
      },
      "source": [
        "# Importing Libraries\n",
        "from __future__ import print_function\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fec795b9c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWLt4t2zu3yq"
      },
      "source": [
        "## **Getting the Data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVwrGntqvUxL"
      },
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6wDZgC0vU2-",
        "outputId": "717f8b93-d908-42d6-8b69-980355afa6d1"
      },
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/jessicali9530/celeba-dataset'\n",
        "od.download(dataset_url)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: kunal1952\n",
            "Your Kaggle Key: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/1.33G [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading celeba-dataset.zip to ./celeba-dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.33G/1.33G [00:23<00:00, 61.9MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbC3fYkjxrj4"
      },
      "source": [
        "# !rm -R celeba-dataset/"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgLqBjmTu3d4",
        "outputId": "7f8f70ef-ef09-458b-dfb7-29ed06cb96f9"
      },
      "source": [
        "DATA_DIR = './celeba-dataset'\n",
        "print(os.listdir(DATA_DIR))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['img_align_celeba', 'list_landmarks_align_celeba.csv', 'list_eval_partition.csv', 'list_attr_celeba.csv', 'list_bbox_celeba.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqSyh7hdxQNT",
        "outputId": "ccaccbe2-b470-49b0-ccbf-348e996f4f88"
      },
      "source": [
        "print(os.listdir(DATA_DIR+'/img_align_celeba/img_align_celeba')[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['189138.jpg', '119906.jpg', '171839.jpg', '195016.jpg', '006318.jpg', '101437.jpg', '086468.jpg', '102716.jpg', '160487.jpg', '155628.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPhv5TqvuZ4H"
      },
      "source": [
        "## **Inputs**\n",
        "\n",
        "Let’s define some inputs for the run:\n",
        "\n",
        "- **dataroot** - the path to the root of the dataset folder. We will talk more about the dataset in the next section\n",
        "- **workers** - the number of worker threads for loading the data with the DataLoader\n",
        "- **batch_size** - the batch size used in training. The DCGAN paper uses a batch size of 128\n",
        "- **image_size** - the spatial size of the images used for training. This implementation defaults to 64x64. If another size is desired, the structures of D and G must be changed. See here for more details\n",
        "- **nc** - number of color channels in the input images. For color images this is 3\n",
        "- **nz** - length of latent vector\n",
        "- **ngf** - relates to the depth of feature maps carried through the generator\n",
        "- **ndf** - sets the depth of feature maps propagated through the discriminator\n",
        "- **num_epochs** - number of training epochs to run. Training for longer will probably lead to better results but will also take much longer\n",
        "- **lr** - learning rate for training. As described in the DCGAN paper, this number should be 0.0002\n",
        "- **beta1** - beta1 hyperparameter for Adam optimizers. As described in paper, this number should be 0.5\n",
        "- **ngpu** - number of GPUs available. If this is 0, code will run in CPU mode. If this number is greater than 0 it will run on that number of GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ZK8YdpuYW7"
      },
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"data/celeba\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tMwQQXQkYrc"
      },
      "source": [
        "## **Exploring the Data**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbJuNyn6vFQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJzKi72PvFTI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muvHy24YvFV_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MAz8e2ivFZI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzCY-fQCvFcY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}